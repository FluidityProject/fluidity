\chapter{Parallel}

\fluidity\ is a fully-parallel program, capable of running on thousands of processors. 
It uses the Message Passing Interface (MPI) library to communicate information between 
processors. Running \fluidity\ in parallel requires that MPI is available on your system
and is properly configured. This chapter assumes that this is the case. The 
terminology for parallel processing can sometime be confusing. Here, we use the 
term \emph{processor} to mean one physical CPU core and \emph{process} is one part
of a parallel instance of \fluidity. Normally, the number of processors used matches
the number of processes, i.e. if \fluidity\ is split into 256 parts, it is run on 256 processors.

\section{Set up}

The first step in running a \fluidity\ set-up in parallel is to create the software
use to decompose the initial mesh into multiple parts, fldecomp. This can be made using:
\begin{lstlisting}[language=bash]
make fltools
\end{lstlisting}
inside your \fluidity\ folder.The fldecomp binary will then be created in the bin/ directory. 
You can then decompose the initial mesh into multiple regions, one per process using the following command
\begin{lstlisting}[language=bash]
fldecomp -n [PARTS] [BASENAME]
\end{lstlisting}
where BASENAME is the triangle mesh base name (excluding extensions). "-i triangle"
instruct fldecomp to perform a triangle-to-triangle decomposition. 
This will create PARTS partition triangle meshes together with PARTS .halo files. 

\subsection{Decomposing a periodic mesh}

To decompose a periodic mesh to be run on four processors simply type:

\begin{lstlisting}[language=bash]
mpiexec -n 4 ./myfluidity/bin/flredecomp \
    -i 1 -o 4 foo_periodised foo_periodised_flredecomp
\end{lstlisting}

Where:

\begin{lstlisting}[language=bash]
mpiexec -n [target number of processors] \
   ./myfluidity/bin/flredecomp -i [input number of processors] \
        -o [target number of processors] [input flml] [output flml]
\end{lstlisting}

The output of running flredecomp is a series of mesh and vtu files as well as the new flml; in this case foo\_periodised\_flredecomp.flml. It is this flml that you use to run a periodic parallel mesh. Therefore, as previously:

\begin{lstlisting}[language=bash]
mpiexec -n [number of processors] \
   ./myfluidity/bin/fluidity [options] foo_periodised_flredecomp.flml
\end{lstlisting}

\section{Running}

To run in parallel there are no further changes needed, apart from running \fluidity
within the mpirun or mpiexec framework. Simply prepend the normal command line with mpiexec:
\begin{lstlisting}[language=bash]
mpiexec -n [PROCESSES] dfluidity -l -v2 my_setup.flml
\end{lstlisting}

\subsection{Running a periodic extruded mesh}

Running a periodic extruded mesh (section \ref{mesh!extruded}) in parallel, requires a number of steps to be taken in order to provide flredecomp (required to make the periodic mesh) with a 2D periodic mesh:

\begin{itemize}
\item Make the periodic 2D flml from the periodic 2plus1 flml
\begin{lstlisting}[language=bash]
 ./myfluidity/bin/make_periodic_2D_flml foo.flml
\end{lstlisting}
\item Periodise the new 2D flml
\begin{lstlisting}[language=bash]
 ./myfluidity/periodise foo_2D.flml
\end{lstlisting}
\item Flredecomp the periodised 2D flml
\begin{lstlisting}[language=bash]
mpiexec -n 4 ./myfluidity/bin/flredecomp \
    -i 1 -o 4 foo_2D_periodised foo_2D_periodised_flredecomp
\end{lstlisting}
\item Periodise the 2plus1 flml
\begin{lstlisting}[language=bash]
 ./myfluidity/periodise foo.flml
\end{lstlisting}
\item Replace the mesh file in foo.flml with the one created in flml\_2D.flml
\item Run the parallel flml as before
\begin{lstlisting}[language=bash]
mpiexec -n [number of processors] \
   ./myfluidity/bin/fluidity [options] foo_periodised.flml
\end{lstlisting}
\end{itemize}

There is only one required mesh: the CoordinateMesh. Some settings, fields or other
settings have specific mesh requirements. This are discussed under the appropriate options.

\subsection{Running in paralell with debugging}

\subsection{Running in the ICT Cluster}

ICT-HPC have now 3 platforms: CX1 (Dell PC Cluster), CX2 (SGI) and AX1 (SGI). \$HOME and \$WORK are shared between the three systems.

The environment of all systems is controlled by the user using [http://modules.sourceforge.net/ environment modules]. The following configuration files are recommended and the remaining instructions assume that these files have been created:

\begin{lstlisting}[language=bash]
$HOME/.bash_profile
\end{lstlisting}

\begin{lstlisting}[language=bash]
 # .bashrc
 
 # Source global definitions
 if [ -f /etc/bashrc ]; then
   . /etc/bashrc
 fi
 
 export MODULEPATH=$MODULEPATH:$HOME/modules
\end{lstlisting}

\begin{lstlisting}[language=bash]
$HOME/.bashrc
\end{lstlisting}

\begin{lstlisting}[language=bash]
 # Get the aliases and functions
 if [ -f ~/.bashrc ]; then
 	. ~/.bashrc
 fi
\end{lstlisting}


\begin{lstlisting}[language=bash]
$HOME/modules/fluidity-cx1
\end{lstlisting}